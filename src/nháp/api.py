from selenium import webdriver
from selenium.webdriver.edge.service import Service
from selenium.webdriver.edge.options import Options
from bs4 import BeautifulSoup
import time
import re
import urllib.parse
import json
import random
import os


# =============================
# C·∫•u h√¨nh tr√¨nh duy·ªát Edge
# =============================
def setup_driver(proxy=None):
    s = Service('./msedgedriver.exe')
    options = Options()
    options.use_chromium = True
    options.add_argument("--headless=new")
    options.add_argument("--window-size=1000,800")
    options.add_argument("--disable-gpu")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--lang=vi-VN")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-blink-features=AutomationControlled")

    ua_list = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/141.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/121.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
    ]
    options.add_argument(f"--user-agent={random.choice(ua_list)}")

    if proxy:
        options.add_argument(f"--proxy-server={proxy}")

    try:
        driver = webdriver.Edge(service=s, options=options)
        driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
            "source": "Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"
        })
        return driver
    except Exception as e:
        print(f"L·ªói kh·ªüi t·∫°o WebDriver: {e}")
        return None


# =============================
# T√°ch c√¢u vƒÉn ti·∫øng Vi·ªát
# =============================
def split_sentences(text, output_file="sentences_list.json", max_len=400):
    text = re.sub(r'\s+', ' ', text.strip())

    sentences = re.split(r'(?<=[.!?])\s+(?=[A-Z√Ä-·ª∏‚Äú(])', text)

    result = []

    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue

        while len(sentence) > max_len or len(urllib.parse.quote(sentence)) > 1800:
            mid = len(sentence) // 2

            # T√¨m ƒëi·ªÉm t√°ch g·∫ßn gi·ªØa ‚Äî ∆∞u ti√™n d·∫•u c√¢u
            split_point = max(
                sentence.rfind('.', 0, mid),
                sentence.rfind(',', 0, mid),
                sentence.rfind(' ', 0, mid)
            )

            # N·∫øu kh√¥ng t√¨m th·∫•y v·ªã tr√≠ ph√π h·ª£p, chia ƒë√¥i th·∫≥ng
            if split_point == -1 or split_point < 50:
                split_point = mid

            part1 = sentence[:split_point + 1].strip()
            part2 = sentence[split_point + 1:].strip()

            print(f"‚ö†Ô∏è C√¢u d√†i, t√°ch l√†m 2 ph·∫ßn:\n  1Ô∏è‚É£ {part1[:100]}...\n  2Ô∏è‚É£ {part2[:100]}...")

            if part1:
                result.append(part1)

            # Ti·∫øp t·ª•c x·ª≠ l√Ω ph·∫ßn c√≤n l·∫°i (c√≥ th·ªÉ v·∫´n d√†i)
            sentence = part2

        if sentence:
            result.append(sentence)

    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    print(f"ƒê√£ t√°ch ƒë∆∞·ª£c {len(result)} c√¢u h·ª£p l·ªá v√† l∆∞u v√†o '{output_file}'.")
    return result


# =============================
# T√¨m ki·∫øm t·ª´ng c√¢u tr√™n Bing
# =============================
def search_sentence(sentence, num_results=5, proxy=None):
    driver = setup_driver(proxy)
    if not driver:
        return [{"sentence": sentence, "results": [], "error": "Kh√¥ng th·ªÉ kh·ªüi t·∫°o WebDriver"}]

    data = [{"sentence": sentence, "results": []}]
    try:
        query = f'"{sentence}"'
        url = f"https://www.bing.com/search?q={query}&count={num_results}&setLang=vi&mkt=vi-VN&cc=VN"

        # Ki·ªÉm tra URL d√†i
        if len(url) > 2048:
            # C√¢u qu√° d√†i => chia ƒë√¥i
            parts = [sentence[:len(sentence)//2], sentence[len(sentence)//2:]]
            print(f"C√¢u qu√° d√†i, chia th√†nh 2 ph·∫ßn:\n{parts[0]}\n{parts[1]}")
            all_data = []
            for part in parts:
                all_data.extend(search_sentence(part, num_results, proxy))
            return all_data

        # --- L·∫ßn t√¨m ki·∫øm ƒë·∫ßu ti√™n (c√≥ ngo·∫∑c k√©p) ---
        driver.get(url)
        time.sleep(random.uniform(3, 6))

        if "captcha" in driver.current_url.lower() or "sorry" in driver.page_source.lower():
            data[0]["error"] = "B·ªã ch·∫∑n CAPTCHA ‚Äì th·ª≠ l·∫°i v·ªõi proxy kh√°c"
            return data

        soup = BeautifulSoup(driver.page_source, "lxml")
        blocks = soup.select("li.b_algo, div.b_algo, div.b_card")[:num_results]

        for block in blocks:
            a = block.find("a", href=True)
            title_tag = block.find("h2")
            snippet_tag = block.find("p")

            link = a["href"] if a else ""
            title = title_tag.get_text(strip=True) if title_tag else ""
            snippet = snippet_tag.get_text(strip=True) if snippet_tag else ""

            if not link or not title:
                continue
            if "sex" in link.lower() or "porn" in link.lower():
                continue

            data[0]["results"].append({
                "title": title,
                "link": link,
                "snippet": snippet,
                "has_plagiarism": False
            })

        # N·∫øu kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£, th·ª≠ l·∫°i kh√¥ng c√≥ ngo·∫∑c k√©p
        if not data[0]["results"]:
            print(f"Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ cho c√¢u (c√≥ ngo·∫∑c k√©p), th·ª≠ l·∫°i kh√¥ng ngo·∫∑c k√©p:\n‚Üí {sentence[:100]}...")
            driver.quit()  # ƒê√≥ng driver c≈©
            time.sleep(random.uniform(2, 4))

            driver_retry = setup_driver(proxy)
            if not driver_retry:
                data[0]["error"] = "Kh√¥ng th·ªÉ kh·ªüi t·∫°o WebDriver khi t√¨m l·∫°i"
                return data

            query2 = urllib.parse.quote(sentence)
            url2 = f"https://www.bing.com/search?q={query2}&count={num_results}&setLang=vi&mkt=vi-VN&cc=VN"
            driver_retry.get(url2)
            time.sleep(random.uniform(3, 6))

            if "captcha" in driver_retry.current_url.lower() or "sorry" in driver_retry.page_source.lower():
                data[0]["error"] = "B·ªã ch·∫∑n CAPTCHA khi th·ª≠ l·∫°i ‚Äì th·ª≠ proxy kh√°c"
                driver_retry.quit()
                return data

            soup2 = BeautifulSoup(driver_retry.page_source, "lxml")
            blocks2 = soup2.select("li.b_algo, div.b_algo, div.b_card")[:num_results]

            for block in blocks2:
                a = block.find("a", href=True)
                title_tag = block.find("h2")
                snippet_tag = block.find("p")

                link = a["href"] if a else ""
                title = title_tag.get_text(strip=True) if title_tag else ""
                snippet = snippet_tag.get_text(strip=True) if snippet_tag else ""

                if not link or not title:
                    continue
                if "sex" in link.lower() or "porn" in link.lower():
                    continue

                data[0]["results"].append({
                    "title": title,
                    "link": link,
                    "snippet": snippet,
                    "has_plagiarism": False,
                    "retry_without_quotes": True
                })

            driver_retry.quit()

        # N·∫øu sau c·∫£ hai l·∫ßn v·∫´n r·ªóng
        if not data[0]["results"]:
            data[0]["error"] = "Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ d√π ƒë√£ th·ª≠ l·∫°i kh√¥ng ngo·∫∑c k√©p"

        return data

    except Exception as e:
        data[0]["error"] = f"L·ªói x·ª≠ l√Ω: {e}"
        return data
    finally:
        try:
            driver.quit()
        except:
            pass


# =============================
# Ch·∫°y to√†n b·ªô pipeline
# =============================
def analyze_serp_structure(query, proxy=None):
    num_results = 5
    all_results = []
    sentences = split_sentences(query)
    if not sentences:
        print("Kh√¥ng c√≥ c√¢u h·ª£p l·ªá.")
        return [], ""

    for i, sentence in enumerate(sentences, 1):
        print(f"\nüîπ C√¢u {i}/{len(sentences)}: {sentence}")
        results = search_sentence(sentence, num_results=num_results, proxy=proxy)
        for res in results:
            all_results.append(res)
            print(json.dumps(res, ensure_ascii=False, indent=4))
        time.sleep(random.uniform(3, 7))

    with open("result.json", "w", encoding="utf-8") as f:
        json.dump(all_results, f, ensure_ascii=False, indent=4)

    print("\nƒê√£ l∆∞u k·∫øt qu·∫£ v√†o result.json")
    return all_results, ""


# =============================
# Ch·∫°y ch√≠nh
# =============================
if __name__ == "__main__":
    try:
        if not os.path.exists("outputtest.txt"):
            print("Kh√¥ng t√¨m th·∫•y file outputtest.txt")
        else:
            with open("outputtest.txt", "r", encoding="utf-8") as f:
                query = f.read().strip()
            analyze_serp_structure(query)
    except Exception as e:
        print(f"L·ªói khi ch·∫°y script: {e}")
